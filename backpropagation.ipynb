{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_hid`` is a list of integer, representing the number of hidden units in each hidden layer.   \n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of hidden layers, ``L``   \n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- Weights $W^{[l]}$ (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l = [2, 3, 2]):\n",
    "        super().__init__() \n",
    "        \n",
    "        \n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l)-1\n",
    "        self.n_l = n_l\n",
    "        \n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i : None for i in range(1, self.L+1)}\n",
    "        self.a = {i : None for i in range(self.L+1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {i : None for i in range(1, self.L+1)}\n",
    "        self.dL_db = {i : None for i in range(1, self.L+1)}\n",
    "\n",
    "        # Our activation functions\n",
    "        self.f = {i : lambda x : torch.tanh(x) for i in range(1, self.L+1)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {\n",
    "            i : lambda x : (1 / (torch.cosh(x)**2)) \n",
    "            for i in range(1, self.L+1)\n",
    "        }\n",
    "        \n",
    "        # fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L+1)})\n",
    "        for i in range(1, self.L+1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i-1], out_features=n_l[i])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L+1):\n",
    "\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i-1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n",
    "\n",
    "A vectorized implementation would be appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #From last layer to the first\n",
    "        for l in range(model.L,0,-1):\n",
    "            \n",
    "            #If current layer is the last, then dz is the loss, else we need dh to compute dz.\n",
    "            if l == model.L:\n",
    "                #The partial derivative of loss with respect to last ouput with formula.\n",
    "                #The dimensions of y_true and y_pred is transposed so it has shape 2x1\n",
    "                dz = ((y_pred - y_true)*2).T\n",
    "            else:\n",
    "                dh = torch.matmul(model.fc[str(l+1)].weight.data.T, dz)\n",
    "                dz = (dh*model.df[l](model.z[l]).T)\n",
    "\n",
    "            db = dz.T[0]\n",
    "            dw = torch.matmul(dz, model.a[l-1])\n",
    "            model.dL_db[l] = db\n",
    "            model.dL_dw[l] = dw\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #From last layer to the first\n",
    "        for l in range(model.L,0,-1):\n",
    "            \n",
    "            #If current layer is the last, then dz is the loss, else we need dh to compute dz.\n",
    "            if l == model.L:\n",
    "                #The partial derivative of loss with respect to last ouput with formula.\n",
    "                #The dimensions of y_true and y_pred is transposed so it has shape 2x1\n",
    "                dz = (-1*(y_true - y_pred)).T\n",
    "            else:\n",
    "                dh = torch.matmul(model.fc[str(l+1)].weight.data.T, dz)\n",
    "                dz = (dh*model.df[l](model.z[l]).T)\n",
    "\n",
    "            db = dz.T[0]\n",
    "            dw = torch.matmul(dz, model.a[l-1])\n",
    "            model.dL_db[l] = db\n",
    "            model.dL_dw[l] = dw\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " ====================== Epoch 1 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.3331, 0.3325, 0.2764, 0.2289, 0.46, 0.5775, 0.8795, 0.9033, 0.9321, 0.9415]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[6.9394e-04, 6.2638e-04],\n",
      "        [5.9461e+00, 5.3672e+00],\n",
      "        [2.2708e-04, 2.0497e-04]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[2.2061e-05, 1.9914e-05],\n",
      "        [1.0659e-01, 9.6214e-02],\n",
      "        [5.9805e-06, 5.3982e-06]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([8.1589e-05, 6.9910e-01, 2.6698e-05])\n",
      "  Autograd's computation:\n",
      " tensor([2.5938e-06, 1.2532e-02, 7.0314e-07])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   54.7836\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   54.7836\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   30.0207\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   30.0207\n",
      "\n",
      " ====================== Epoch 2 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.7619, 0.8716, 0.9094, 0.9404, 0.9493, 0.9543, 0.9581, 0.9617, 0.965, 0.9684]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[9.2206e-04, 8.3230e-04],\n",
      "        [3.3193e+00, 2.9961e+00],\n",
      "        [2.9818e-04, 2.6915e-04]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[1.5061e-05, 1.3595e-05],\n",
      "        [4.3398e-02, 3.9173e-02],\n",
      "        [4.4442e-06, 4.0116e-06]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([1.0841e-04, 3.9026e-01, 3.5057e-05])\n",
      "  Autograd's computation:\n",
      " tensor([1.7708e-06, 5.1024e-03, 5.2252e-07])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   75.4844\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   75.4844\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   60.3388\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   60.3388\n",
      "\n",
      " ====================== Epoch 3 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.8103, 0.9093, 0.9403, 0.9636, 0.9696, 0.9724, 0.9738, 0.9753, 0.9765, 0.9781]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[1.1812e-03, 1.0662e-03],\n",
      "        [2.4315e+00, 2.1947e+00],\n",
      "        [3.6169e-04, 3.2647e-04]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[1.3237e-05, 1.1948e-05],\n",
      "        [2.3891e-02, 2.1565e-02],\n",
      "        [3.8226e-06, 3.4504e-06]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([1.3888e-04, 2.8587e-01, 4.2524e-05])\n",
      "  Autograd's computation:\n",
      " tensor([1.5563e-06, 2.8089e-03, 4.4943e-07])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   100.7755\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   100.7755\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   88.6813\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   88.6813\n",
      "\n",
      " ====================== Epoch 4 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.8329, 0.9255, 0.9534, 0.9736, 0.9779, 0.9799, 0.9807, 0.9815, 0.9821, 0.9831]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[1.4893e-03, 1.3443e-03],\n",
      "        [2.0069e+00, 1.8115e+00],\n",
      "        [4.2744e-04, 3.8583e-04]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[1.2814e-05, 1.1566e-05],\n",
      "        [1.5790e-02, 1.4253e-02],\n",
      "        [3.5279e-06, 3.1844e-06]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([1.7510e-04, 2.3595e-01, 5.0256e-05])\n",
      "  Autograd's computation:\n",
      " tensor([1.5066e-06, 1.8565e-03, 4.1478e-07])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   126.0951\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   126.0951\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   115.8488\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   115.8488\n",
      "\n",
      " ====================== Epoch 5 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.8456, 0.9345, 0.9607, 0.9791, 0.9824, 0.984, 0.9846, 0.9852, 0.9855, 0.9862]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[1.8640e-03, 1.6825e-03],\n",
      "        [1.7605e+00, 1.5891e+00],\n",
      "        [4.9927e-04, 4.5066e-04]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[1.3086e-05, 1.1812e-05],\n",
      "        [1.1581e-02, 1.0454e-02],\n",
      "        [3.3976e-06, 3.0668e-06]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([2.1916e-04, 2.0699e-01, 5.8700e-05])\n",
      "  Autograd's computation:\n",
      " tensor([1.5386e-06, 1.3616e-03, 3.9947e-07])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   151.0132\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   151.0132\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   142.1616\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   142.1616\n",
      "\n",
      " TEST FAILED: Gradients NOT consistent with autograd's computations.\n",
      "\n",
      " TEST FAILED: Gradients NOT consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "tensor([[-0.4352, -0.3434],\n",
      "        [ 0.1495, -0.4138],\n",
      "        [-0.6683, -0.1448]])\n",
      "tensor([-0.0597, -0.4707,  0.1095])\n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 2 / 4: SOME TESTS FAILED, use 'verbose=True' and check the output for more details\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "\n",
    "main_test(backpropagation, model, verbose=True, data='toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " ====================== Epoch 1 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0247, 0.9944, 0.9894, 0.9937, 0.9935]\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   312.6307\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   312.6307\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   275.5594\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   275.5594\n",
      "\n",
      " TEST FAILED: Gradients NOT consistent with autograd's computations.\n",
      "\n",
      " TEST FAILED: Gradients NOT consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 2 / 4: SOME TESTS FAILED, use 'verbose=True' and check the output for more details\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MyNet([24*24, 16, 10])\n",
    "main_test(backpropagation, model, verbose=False, data='mnist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[57.3267, 57.3267],\n",
       "        [57.3447, 57.3429],\n",
       "        [57.2166, 57.2173]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(torch.tensor([[ 1.6216e-04,  1.8000e-04],\n",
    "        [-1.3800e-02, -1.5318e-02],\n",
    "        [ 1.2963e-05,  1.4389e-05]]), torch.tensor([[ 2.8287e-06,  3.1399e-06],\n",
    "        [-2.4065e-04, -2.6713e-04],\n",
    "        [ 2.2656e-07,  2.5148e-07]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9336581458986851, 0.9336932680951577)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.3989e-04/1.4983e-04, 1.1068e-04/1.1854e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_layers_param():\n",
    "    for l in range(1,model.L+1):\n",
    "        print('\\nLayer: ', l)\n",
    "        print('weights: ', model.fc[str(l)].weight.data)\n",
    "        print('bias: ', model.fc[str(l)].bias.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer:  1\n",
      "weights:  tensor([[ 0.1147, -0.2399],\n",
      "        [ 0.0448, -0.6335],\n",
      "        [-0.1786, -0.6912]])\n",
      "bias:  tensor([-0.0807, -0.2581, -0.5455])\n",
      "\n",
      "Layer:  2\n",
      "weights:  tensor([[-0.1520, -0.3967,  0.1105],\n",
      "        [ 0.5335, -0.3719, -0.1566]])\n",
      "bias:  tensor([ 0.2745, -0.1143])\n"
     ]
    }
   ],
   "source": [
    "show_layers_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 0.9760, 0.9745],\n",
       "         [0.9784, 0.7312, 0.9986]]),\n",
       " tensor([0.9458, 0.9909]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.df[1](model.fc[str(2)].weight.data), model.df[1](model.fc[str(2)].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9857, 0.9996, 0.8158])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.df[1](model.fc[str(1)].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5002, -3.5263]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.z[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for l in range(model.L,0,-1):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7809, 0.8896]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.df[1](model.z[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor([[-0.6030, -0.7212]]),\n",
       " 1: tensor([[-0.2602,  0.0526, -0.1992]], grad_fn=<TanhBackward0>),\n",
       " 2: tensor([[-0.4681, -0.3323]], grad_fn=<TanhBackward0>)}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.df[2](model.z[2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3964, -0.3073]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.z[2]* model.df[2](model.z[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.a[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0036, -0.0034]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.a[2]* (model.df[2](model.z[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: tensor([0.4302]), 2: tensor([0.6775])}"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (1): Linear(in_features=2, out_features=3, bias=True)\n",
       "  (2): Linear(in_features=3, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_TensorBase.dim() takes no arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[157], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: _TensorBase.dim() takes no arguments (1 given)"
     ]
    }
   ],
   "source": [
    "model.fc[str(1)].bias.data.dim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "matmul() missing 2 required positional argument: \"input\", \"other\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[174], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: matmul() missing 2 required positional argument: \"input\", \"other\""
     ]
    }
   ],
   "source": [
    "torch.matmul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor([[ 0.7015, -2.4287]]),\n",
       " 1: tensor([[-0.5260,  0.1538, -0.2480]], grad_fn=<TanhBackward0>),\n",
       " 2: tensor([[ 0.3936, -0.5398]], grad_fn=<TanhBackward0>)}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: tensor([[ 0.3640, -0.2106,  0.0463]], grad_fn=<AddmmBackward0>),\n",
       " 2: tensor([[0.0940, 0.2982]], grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc[str(2)].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: tensor([[-0.0161, -0.0172],\n",
       "         [ 0.0050,  0.0053],\n",
       "         [-0.2129, -0.2269]]),\n",
       " 2: tensor([[-16.0031,  16.0033, -15.9934],\n",
       "         [ 16.0031, -16.0034,  15.9934]])}"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dL_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet([2, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer:  1\n",
      "weights:  tensor([[ 0.0472, -0.6182],\n",
      "        [ 0.6941,  0.2990],\n",
      "        [ 0.5138, -0.0120]])\n",
      "bias:  tensor([-0.2873,  0.5716, -0.0682])\n",
      "\n",
      "Layer:  2\n",
      "weights:  tensor([[-0.3801, -0.5443,  0.2979],\n",
      "        [-0.2486, -0.4878, -0.2317]])\n",
      "bias:  tensor([0.0515, 0.5273])\n"
     ]
    }
   ],
   "source": [
    "show_layers_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0612,  0.1574]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(torch.ones(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor([[1., 1.]]),\n",
       " 1: tensor([[-0.6953,  0.9162,  0.4082]], grad_fn=<TanhBackward0>),\n",
       " 2: tensor([[-0.0612,  0.1574]], grad_fn=<TanhBackward0>)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: tensor([[-0.8582,  1.5647,  0.4335]], grad_fn=<AddmmBackward0>),\n",
       " 2: tensor([[-0.0612,  0.1587]], grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9963, 0.9752]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.df[2](model.z[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8582,  1.5647,  0.4335], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.z[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc[str(2)].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x2 and 1x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dh2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m dh2\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 1x2)"
     ]
    }
   ],
   "source": [
    "dh2 = torch.matmul(model.fc[str(2)].weight.data.T, model.z[2])\n",
    "dh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5165],\n",
       "        [0.1606],\n",
       "        [0.8333]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.df[2](model.z[1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0082, -0.0025, -0.0132],\n",
       "        [-0.0225, -0.0070, -0.0362],\n",
       "        [-0.0282, -0.0088, -0.0456]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz2 = torch.mul(dh2, model.df[1](model.z[1]))\n",
    "dz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x3 and 2x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dw2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdz2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m dw2\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x3 and 2x1)"
     ]
    }
   ],
   "source": [
    "dw2 = torch.matmul(dz2, model.a[2].T)\n",
    "dw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([-5.8622e-06,  1.6592e-06, -7.5508e-05]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 0, 3]),\n",
       " tensor([[1],\n",
       "         [0],\n",
       "         [3]]))"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,0,3])\n",
    "c = torch.reshape(a, (3,1))\n",
    "a,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4200, 1.0000, 0.0099])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = model.df[1](a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4200, 0.0000, 0.0296])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
